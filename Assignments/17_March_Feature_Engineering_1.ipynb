{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91601e8f",
   "metadata": {},
   "source": [
    "## **Q1- What are missing values in a dataset? why is it essential to handel missing values? Name some algorithms that are not affected by missing values**\n",
    "\n",
    "ans: Missing values occur in a dataset when some information is not stored for a variable.\n",
    "\n",
    "There are 3 types of missing values:-\n",
    "- 1) Missing Completely at Random(MCAR): probability of a value being missing is unrelated to both observed and missing data.Ex missing data inside a survey of certain disease might be MCAR if missing response is not related to the disease status\n",
    "- 2) Missing at Random(MAR):probability of missing value depends on the observed data. Ex.Suppose the in an income data isf the missing value is due to a person's age or gender and not because of their income level then it is MAR.\n",
    "- 3) Missing Data Not at Random(MNAR):probability of missing value depends in the value of missing data itself.ex:suppose you are collecting data on the income and job satisfaction of employees in a company. If employees who are less satisfied with their jobs are more likely to refuse to report their income, then the data is not missing at random.\n",
    "\n",
    "why is it essential to handel missing values?\n",
    "- To ensure accuracy of the analsis\n",
    "- To avoid bias in the result\n",
    "- to improve model efficiency\n",
    "\n",
    "Algorithms not affected by missing values:-\n",
    "- Decision Tree \n",
    "- Random Forests\n",
    "- Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5a466",
   "metadata": {},
   "source": [
    "## **Q2 List down techniques used to handel missing data. Give an example of each with python code**\n",
    "Techniques to Handle missing data:-\n",
    "- 1 Deletion\n",
    "- 2 Impution (mean,median, mode)\n",
    "- 3 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63875b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a  b     c\n",
      "0  1.0  5   9.0\n",
      "1  2.0  6  10.0\n",
      "3  4.0  8  12.0\n",
      "   b\n",
      "0  5\n",
      "1  6\n",
      "2  7\n",
      "3  8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**1- Deletion:** This involves removing all rows or columns that contain missing values. This can be done using the dropna() function from the pandas library:\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#row deletion\n",
    "data = pd.DataFrame({'a': [1, 2, np.nan, 4], 'b': [5, 6, 7, 8], 'c': [9, 10, np.nan, 12]})\n",
    "data = data.dropna()\n",
    "print(data)\n",
    "\n",
    "#column deletion\n",
    "data = pd.DataFrame({'a': [1, 2, np.nan, 4], 'b': [5, 6, 7, 8], 'c': [9, 10, np.nan, 12]})\n",
    "data = data.dropna(axis=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e851bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value impution: \n",
      "          a  b          c\n",
      "0  1.000000  5   9.000000\n",
      "1  2.000000  6  10.000000\n",
      "2  2.333333  7  10.333333\n",
      "3  4.000000  8  12.000000\n",
      "\n",
      "median value impution: \n",
      "     a  b     c\n",
      "0  1.0  5   9.0\n",
      "1  2.0  6  10.0\n",
      "2  2.0  7  10.0\n",
      "3  4.0  8  12.0\n",
      "\n",
      "mode value impution: \n",
      "     a  b     c\n",
      "0  1.0  5   9.0\n",
      "1  2.0  6  10.0\n",
      "2  4.0  7  12.0\n",
      "3  4.0  8  12.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "**2- Impution:** This involves replacing missing values with estimated values. This can be done using a variety of methods, such as the mean, median, or mode of the remaining values in the column.\n",
    "\"\"\"\n",
    "\n",
    "#mean value impution:\n",
    "data = pd.DataFrame({'a': [1, 2, np.nan, 4], 'b': [5, 6, 7, 8], 'c': [9, 10, np.nan, 12]})\n",
    "data = data.fillna(data.mean())\n",
    "print(f\"mean value impution: \\n{data}\\n\")\n",
    "\n",
    "#median value impution:\n",
    "data = pd.DataFrame({'a': [1, 2, np.nan, 4], 'b': [5, 6, 7, 8], 'c': [9, 10, np.nan, 12]})\n",
    "data = data.fillna(data.median())\n",
    "print(f\"median value impution: \\n{data}\\n\")\n",
    "\n",
    "#mode value impution:\n",
    "data = pd.DataFrame({'a': [1, 2, np.nan, 4], 'b': [5, 6, 7, 8], 'c': [9, 10, np.nan, 12]})\n",
    "data = data.fillna(data.mode())\n",
    "print(f\"mode value impution: \\n{data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89498475",
   "metadata": {},
   "source": [
    "**3- Modeling:** involves using a machine learning model to predict the missing values. This can be done using a variety of algorithms, such as linear regression or decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33025696",
   "metadata": {},
   "source": [
    "## **Q3 Explain the imbalanced data .What will happen if imbalanced data is not handeled?**\n",
    "imbalanced data is a datatset in which number of observation in one type of class is significantly larger than the number of observations in another class \n",
    "\n",
    "Drawbacks of having imbalanced data set of model training:\n",
    "- Model will be biased towards majority class\n",
    "- Model will have accuracy in pridicting majority class but not very good at pridicting minority class\n",
    "- Less interpretable cuz will more likely include majority class features only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fef57",
   "metadata": {},
   "source": [
    "## **Q4 What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.**<br>\n",
    "They are  the techniques used to change the size of the dataset \n",
    "Upscaling increasees the size of the minority dataset.\n",
    "Downscaling decreases the size of the majority dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f2494bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    900\n",
      "1    100\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>2.513590</td>\n",
       "      <td>3.550477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1.273161</td>\n",
       "      <td>2.019777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1.741623</td>\n",
       "      <td>2.576725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>3.551299</td>\n",
       "      <td>3.533925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1.574519</td>\n",
       "      <td>1.885761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1  feature2  target\n",
       "1795  2.513590  3.550477       1\n",
       "1796  1.273161  2.019777       1\n",
       "1797  1.741623  2.576725       1\n",
       "1798  3.551299  3.533925       1\n",
       "1799  1.574519  1.885761       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UPscaling example using sklearn.utile.resample\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class0 = pd.DataFrame({\n",
    "    'feature1': np.random.normal(loc=0,scale=1,size=900),\n",
    "    'feature2': np.random.normal(loc=0,scale=1,size=900),\n",
    "    'target': [0]*900\n",
    "})\n",
    "\n",
    "class1 = pd.DataFrame({\n",
    "    'feature1': np.random.normal(loc=2,scale=1,size=100),\n",
    "    'feature2': np.random.normal(loc=2,scale=1,size=100),\n",
    "    'target': [1]*100\n",
    "})\n",
    "df = pd.concat([class0,class1]).reset_index(drop=True)\n",
    "print(df.target.value_counts())\n",
    "\n",
    "#upsampeling 1\n",
    "df_majority = df[df['target']==0]\n",
    "df_minority = df[df['target']==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority,replace=True, n_samples=len(df_majority),random_state=4248)\n",
    "df_upsampled = pd.concat([df_majority,df_minority_upsampled]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba94167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-1.296287</td>\n",
       "      <td>0.749574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.738141</td>\n",
       "      <td>0.823435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.412770</td>\n",
       "      <td>-0.776528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-1.192747</td>\n",
       "      <td>-0.354780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.556827</td>\n",
       "      <td>-1.561671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  target\n",
       "195 -1.296287  0.749574       0\n",
       "196  1.738141  0.823435       0\n",
       "197 -0.412770 -0.776528       0\n",
       "198 -1.192747 -0.354780       0\n",
       "199  0.556827 -1.561671       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downsampling using resample \n",
    "df_majority_downsampled = resample(df_majority,\n",
    "         replace=False, #becausee we need to reduce datapoints\n",
    "        n_samples=len(df_minority),\n",
    "        random_state=42)\n",
    "df_downsampled = pd.concat([df_minority,df_majority_downsampled]).reset_index(drop=True)\n",
    "df_downsampled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf732a2",
   "metadata": {},
   "source": [
    "## **Q5-What is data Augmentation? Explain SMOTE.**\n",
    "\n",
    "**Data Augmentation:** Itis a technique used to artificially increase the size of the dataset. useful if the dataset is imbalanced to increase the performance of the model.\n",
    " \n",
    "**SMOTE((Synthetic Minority Over-sampling Technique)):** One of the most popular data augmentation technique. It involves generating synthetic instances of the minority class by interpolating between the existing instances, using library imblearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ac726",
   "metadata": {},
   "source": [
    "# **Q6- What are outliers in a dataset? Why is it essential to handle outliers?**\n",
    "\n",
    "Outliers are data points that are far away from the rest of the data points in a dataset. They can be caused by a variety of factors, such as:\n",
    "\n",
    "- Data entry errors\n",
    "- Equipment malfunctions\n",
    "- Natural variation\n",
    "- Intentional fraud\n",
    "\n",
    "Outliers can have a significant impact on the analysis of a dataset. They can:\n",
    "- Bias the results of statistical tests\n",
    "- Make it difficult to identify patterns in the data\n",
    "- Make it difficult to build accurate machine learning models\n",
    "\n",
    "It is essential to handle outliers in a dataset to ensure that the results of the analysis are accurate. There are a number of different ways to handle outliers, such as:\n",
    "- **Deleting outliers:** This involves removing the outliers from the dataset. This can be a good option if the outliers are clearly incorrect or if they are not representative of the overall population.\n",
    "- **Imputing outliers:** This involves replacing the outliers with estimated values. This can be done using a variety of methods, such as the mean, median, or mode of the remaining values in the dataset. Imputation is a good option if the outliers are likely to be correct but are simply not representative of the overall population.\n",
    "- **Modeling outliers:** This involves using a machine learning model to predict the outliers. This can be a good option if the outliers are related to other variables in the dataset.\n",
    "\n",
    "The best way to handle outliers will depend on the specific dataset and the analysis that is being performed. There is no one-size-fits-all solution.\n",
    "\n",
    "Here are some of the reasons why it is essential to handle outliers:\n",
    "\n",
    "- Outliers can bias the results of statistical tests. For example, if a dataset has a few very high outliers, the mean and standard deviation of the dataset will be artificially inflated. This can lead to incorrect conclusions about the distribution of the data.\n",
    "- Outliers can make it difficult to identify patterns in the data. For example, if a dataset has a few very high outliers, it can be difficult to see the overall trend of the data.\n",
    "- Outliers can make it difficult to build accurate machine learning models. Machine learning models are trained on data, and if the data contains outliers, the model will be biased towards the outliers. This can lead to inaccurate predictions.\n",
    "\n",
    "By handling outliers, we can ensure that the results of our analysis are accurate and that our machine learning models are effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715da324",
   "metadata": {},
   "source": [
    "## **Q7- You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?**\n",
    "Techniques to Handle missing data:-\n",
    "\n",
    "1. Deletion\n",
    "2. Impution (mean,median, mode)\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021e3bd",
   "metadata": {},
   "source": [
    "## **Q8- You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?**\n",
    "\n",
    "\n",
    "Here are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "- **Visualize the missing data.** A simple way to get an idea of the pattern of missing data is to visualize it. You can use a heatmap or a scatterplot to see if there are any patterns in the missing data. For example, if the missing data is concentrated in certain rows or columns, then it is likely that there is a pattern to the missing data.\n",
    "- **Run statistical tests.** There are a number of statistical tests that can be used to determine if the missing data is missing at random. One common test is the Little's test of missing completely at random (MCAR). This test compares the distribution of missing data to the distribution of observed data. If the two distributions are similar, then the missing data is likely to be missing at random.\n",
    "- **Use a machine learning model.** A machine learning model can be used to predict the missing data. If the missing data is missing at random, then the machine learning model will not be able to predict the missing data with any accuracy. However, if the missing data is not missing at random, then the machine learning model will be able to predict the missing data with some accuracy.\n",
    "\n",
    "It is important to note that no single strategy can definitively determine if the missing data is missing at random or if there is a pattern to the missing data. However, by using a combination of strategies, you can get a better understanding of the pattern of missing data and make an informed decision about how to handle it.\n",
    "\n",
    "Here are some additional things to consider when determining the pattern of missing data:\n",
    "\n",
    "- **The type of data.** Some types of data are more likely to be missing at random than others. For example, categorical data is more likely to be missing at random than numerical data.\n",
    "- **The context of the data.** The context of the data can also affect the pattern of missing data. For example, if the data is collected from a survey, the missing data may be more likely to be related to the respondents' willingness to answer certain questions.\n",
    "- **The reason for the missing data.** The reason for the missing data can also affect the pattern of missing data. For example, if the data is missing due to a data entry error, the missing data may be more likely to be random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a57de",
   "metadata": {},
   "source": [
    "## **Q9- Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?**\n",
    "\n",
    "here are some strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "- **Use a confusion matrix.** A confusion matrix is a table that shows the true positives, false positives, true negatives, and false negatives for a machine learning model. This can be helpful for understanding the performance of the model on both the majority and minority classes.\n",
    "- **Calculate the precision and recall.** Precision and recall are two metrics that can be used to evaluate the performance of a machine learning model on a binary classification task. Precision measures the proportion of positive predictions that are actually positive, while recall measures the proportion of actual positives that are correctly predicted as positive. In an imbalanced dataset, it is important to consider both precision and recall, as a model that has high precision may not be able to identify enough of the minority class, while a model with high recall may make too many false positives.\n",
    "- **Use the F1 score.** The F1 score is a weighted average of precision and recall. It is a more balanced metric than either precision or recall, and it is often used as a single metric to evaluate the performance of a machine learning model on an imbalanced dataset.\n",
    "- **Use a cost-sensitive learning algorithm.** Cost-sensitive learning algorithms are designed to take into account the imbalance in the dataset. These algorithms can be used to improve the performance of the model on the minority class.\n",
    "\n",
    "It is important to note that no single strategy is perfect for evaluating the performance of a machine learning model on an imbalanced dataset. The best strategy will depend on the specific dataset and the goals of the project. However, by using a combination of strategies, you can get a better understanding of the performance of the model and make an informed decision about its effectiveness.\n",
    "\n",
    "Here are some additional things to consider when evaluating the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "- **The target metric.** The target metric is the metric that you are most interested in. For example, you might be interested in the accuracy of the model, the precision of the model, or the recall of the model.\n",
    "- **The cost of false positives and false negatives.** The cost of false positives and false negatives may not be equal. For example, in a medical diagnosis setting, a false positive might be less costly than a false negative.\n",
    "- **The imbalance ratio.** The imbalance ratio is the ratio of the number of instances in the majority class to the number of instances in the minority class. The higher the imbalance ratio, the more difficult it will be to achieve good performance on the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb884f",
   "metadata": {},
   "source": [
    "## **Q10- When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?**\n",
    "\n",
    "using random downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5205f88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.837924</td>\n",
       "      <td>2.430862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.547669</td>\n",
       "      <td>2.466140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.348208</td>\n",
       "      <td>3.374865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.658562</td>\n",
       "      <td>1.665740</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972649</td>\n",
       "      <td>3.475194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-1.296287</td>\n",
       "      <td>0.749574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.738141</td>\n",
       "      <td>0.823435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.412770</td>\n",
       "      <td>-0.776528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-1.192747</td>\n",
       "      <td>-0.354780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.556827</td>\n",
       "      <td>-1.561671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  target\n",
       "0    2.837924  2.430862       1\n",
       "1   -0.547669  2.466140       1\n",
       "2    3.348208  3.374865       1\n",
       "3    1.658562  1.665740       1\n",
       "4    0.972649  3.475194       1\n",
       "..        ...       ...     ...\n",
       "195 -1.296287  0.749574       0\n",
       "196  1.738141  0.823435       0\n",
       "197 -0.412770 -0.776528       0\n",
       "198 -1.192747 -0.354780       0\n",
       "199  0.556827 -1.561671       0\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "#Downsampling using resample \n",
    "df_majority_downsampled = resample(df_majority,\n",
    "         replace=False, #becausee we need to reduce datapoints\n",
    "        n_samples=len(df_minority),\n",
    "        random_state=42)\n",
    "df_downsampled = pd.concat([df_minority,df_majority_downsampled]).reset_index(drop=True)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52368d9",
   "metadata": {},
   "source": [
    "## **Q11- You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?**\n",
    "\n",
    "using\n",
    "- **Random Upsampling**\n",
    "- **SMOTE(Synthetic Minority Over-Sampling Technique)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
